{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Make_models for resnet_50-2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "drgAzT6stRFO",
        "outputId": "89ed640c-d82c-4cc5-df1e-c4f6e01b083c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPtFCBQ4h_dE"
      },
      "source": [
        "### image dataset load"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAncwh--h-gZ",
        "outputId": "da1b2610-3dc8-491c-c674-1cb76b4d0223",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "\n",
        "data_dir = r'/content/drive/My Drive/기업프로젝트-라젠/data/40, 320'\n",
        "\n",
        "IMAGE_SIZE = (224,224)\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    data_dir,\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=42,\n",
        "    image_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE)\n",
        "\n",
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    data_dir,\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=42,\n",
        "    image_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 13120 files belonging to 320 classes.\n",
            "Using 10496 files for training.\n",
            "Found 13120 files belonging to 320 classes.\n",
            "Using 2624 files for validation.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZVbV9RXkHZf",
        "outputId": "90400adc-90a3-47cf-cd0e-eab6d278380d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(train_ds)\n",
        "print(val_ds)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<BatchDataset shapes: ((None, 224, 224, 3), (None,)), types: (tf.float32, tf.int32)>\n",
            "<BatchDataset shapes: ((None, 224, 224, 3), (None,)), types: (tf.float32, tf.int32)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ValGOd6NueuI"
      },
      "source": [
        "### 데이터 정규화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exuXOu_1kmUW",
        "outputId": "87c17b2d-e26d-4378-eb28-88332ec955e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "# 0~1 사이의 값을 가지도록 조정\n",
        "def normalization_dataset(dataset):\n",
        "    normalization_layer = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)\n",
        "    normalized_ds = dataset.map(lambda x, y: (normalization_layer(x), y))\n",
        "    return normalized_ds \n",
        "\n",
        "train_ds = normalization_dataset(train_ds)\n",
        "val_ds = normalization_dataset(val_ds) \n",
        "print(train_ds)\n",
        "print(val_ds)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<MapDataset shapes: ((None, 224, 224, 3), (None,)), types: (tf.float32, tf.int32)>\n",
            "<MapDataset shapes: ((None, 224, 224, 3), (None,)), types: (tf.float32, tf.int32)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4m96NFyywTUa"
      },
      "source": [
        "### 성능을위한 데이터 세트 구성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGr1cOwMlkXN"
      },
      "source": [
        "# cache() 첫 번째 에포크 동안 디스크에서로드 된 이미지를 메모리에 보관합니다. \n",
        "# prefetch 학습 중에 데이터 전처리 및 모델 실행과 겹칩니다\n",
        "\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mYp3Tq7iIAl"
      },
      "source": [
        "\n",
        "### tensorflow module load"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btD0TNg4cmZh"
      },
      "source": [
        "import tensorflow_hub as hub\n",
        "import tensorflow_addons as tfa\n",
        " \n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Input, Dense, Flatten, Dropout, Lambda, MaxPooling2D, Conv2D\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldcSOvdqiMOx"
      },
      "source": [
        "### 모델 load, train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IJeX4HLcnCI",
        "outputId": "d1c5100a-d443-4bf5-be1f-df0a303c4578",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# resnet_50\n",
        "hub_url = \"https://tfhub.dev/tensorflow/resnet_50/classification/1\"\n",
        "hub_layer = hub.KerasLayer(hub_url,\n",
        "                           input_shape=IMAGE_SIZE + (3,),\n",
        "                           trainable=True)\n",
        "\n",
        "# model구축\n",
        "# triplets 128벡터값을 위한 dense출력값을 128로 설정, activation = None\n",
        "# deep architecture이후 l2정규화\n",
        "model = Sequential()\n",
        "model.add(hub_layer)\n",
        "model.add(Dense(128, activation=None))\n",
        "model.add(Lambda(lambda x: tf.math.l2_normalize(x, axis=1)))  \n",
        "\n",
        "model.summary()\n",
        "\n",
        "# 모델 complie\n",
        "adam = Adam(lr=0.001, decay=1e-6)\n",
        "model.compile(optimizer=adam, loss=tfa.losses.TripletSemiHardLoss())\n",
        "\n",
        "# SGD = SGD(lr=0.05)\n",
        "# model.compile(optimizer=SGD, loss=tfa.losses.TripletSemiHardLoss())\n",
        "\n",
        "# Adagrad = Adagrad(lr=0.01, epsilon=1e-6)\n",
        "# model.compile(optimizer=adam, loss=tfa.losses.TripletSemiHardLoss())\n",
        "\n",
        "# earlystopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)\n",
        "\n",
        "# 모델 학습\n",
        "history = model.fit(train_ds,\n",
        "                    validation_data = val_ds,\n",
        "                    epochs=1000,\n",
        "                    callbacks=[early_stopping]\n",
        "                    )  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "keras_layer_1 (KerasLayer)   (None, 1001)              25612201  \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 128)               128256    \n",
            "_________________________________________________________________\n",
            "lambda_1 (Lambda)            (None, 128)               0         \n",
            "=================================================================\n",
            "Total params: 25,740,457\n",
            "Trainable params: 25,687,337\n",
            "Non-trainable params: 53,120\n",
            "_________________________________________________________________\n",
            "Epoch 1/1000\n",
            " 2/82 [..............................] - ETA: 25s - loss: 1.4469WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.2321s vs `on_train_batch_end` time: 0.3929s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.2321s vs `on_train_batch_end` time: 0.3929s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "82/82 [==============================] - 63s 764ms/step - loss: 1.5109 - val_loss: 1.5203\n",
            "Epoch 2/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.5196 - val_loss: 1.4997\n",
            "Epoch 3/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.4719 - val_loss: 1.4219\n",
            "Epoch 4/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.4670 - val_loss: 1.4681\n",
            "Epoch 5/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.4576 - val_loss: 1.4459\n",
            "Epoch 6/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.4372 - val_loss: 1.4497\n",
            "Epoch 7/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.4176 - val_loss: 1.4119\n",
            "Epoch 8/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.3949 - val_loss: 1.3819\n",
            "Epoch 9/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.3668 - val_loss: 1.3538\n",
            "Epoch 10/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.3474 - val_loss: 1.3299\n",
            "Epoch 11/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.3254 - val_loss: 1.3107\n",
            "Epoch 12/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.2948 - val_loss: 1.3165\n",
            "Epoch 13/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.3048 - val_loss: 1.2954\n",
            "Epoch 14/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.2876 - val_loss: 1.2766\n",
            "Epoch 15/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.2667 - val_loss: 1.2614\n",
            "Epoch 16/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.2561 - val_loss: 1.2589\n",
            "Epoch 17/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.2565 - val_loss: 1.2589\n",
            "Epoch 18/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.3205 - val_loss: 1.3414\n",
            "Epoch 19/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.4078 - val_loss: 1.4009\n",
            "Epoch 20/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.3951 - val_loss: 1.3959\n",
            "Epoch 21/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.3708 - val_loss: 1.3546\n",
            "Epoch 22/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.3455 - val_loss: 1.3468\n",
            "Epoch 23/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.3443 - val_loss: 1.3413\n",
            "Epoch 24/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.3263 - val_loss: 1.3223\n",
            "Epoch 25/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.3413 - val_loss: 1.3733\n",
            "Epoch 26/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.3548 - val_loss: 1.3317\n",
            "Epoch 27/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.3249 - val_loss: 1.3150\n",
            "Epoch 28/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.3083 - val_loss: 1.2855\n",
            "Epoch 29/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.2926 - val_loss: 1.2829\n",
            "Epoch 30/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.2760 - val_loss: 1.2653\n",
            "Epoch 31/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.2698 - val_loss: 1.2882\n",
            "Epoch 32/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.2865 - val_loss: 1.2852\n",
            "Epoch 33/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.2798 - val_loss: 1.1254\n",
            "Epoch 34/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.3263 - val_loss: 1.3181\n",
            "Epoch 35/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.3201 - val_loss: 1.2983\n",
            "Epoch 36/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.2943 - val_loss: 1.3274\n",
            "Epoch 37/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.4508 - val_loss: 1.5072\n",
            "Epoch 38/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.4883 - val_loss: 1.4715\n",
            "Epoch 39/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.4674 - val_loss: 1.4579\n",
            "Epoch 40/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.4564 - val_loss: 1.4592\n",
            "Epoch 41/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.5386 - val_loss: 1.6428\n",
            "Epoch 42/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.6129 - val_loss: 1.5858\n",
            "Epoch 43/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.5592 - val_loss: 1.5321\n",
            "Epoch 44/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.5036 - val_loss: 1.4775\n",
            "Epoch 45/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.4575 - val_loss: 1.4411\n",
            "Epoch 46/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.4256 - val_loss: 1.4183\n",
            "Epoch 47/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.3989 - val_loss: 1.3805\n",
            "Epoch 48/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.3582 - val_loss: 1.3575\n",
            "Epoch 49/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.3284 - val_loss: 1.3179\n",
            "Epoch 50/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.3063 - val_loss: 1.2939\n",
            "Epoch 51/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.2815 - val_loss: 1.2711\n",
            "Epoch 52/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.2641 - val_loss: 1.2600\n",
            "Epoch 53/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.2992 - val_loss: 1.2992\n",
            "Epoch 54/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.2857 - val_loss: 1.2837\n",
            "Epoch 55/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.2703 - val_loss: 1.2538\n",
            "Epoch 56/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.2724 - val_loss: 1.2660\n",
            "Epoch 57/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.2603 - val_loss: 1.2470\n",
            "Epoch 58/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.2431 - val_loss: 1.2648\n",
            "Epoch 59/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.2788 - val_loss: 1.3118\n",
            "Epoch 60/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.3592 - val_loss: 1.3925\n",
            "Epoch 61/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.4000 - val_loss: 1.4331\n",
            "Epoch 62/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.4276 - val_loss: 1.4029\n",
            "Epoch 63/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.4009 - val_loss: 1.3935\n",
            "Epoch 64/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.3890 - val_loss: 1.3753\n",
            "Epoch 65/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.3582 - val_loss: 1.3382\n",
            "Epoch 66/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.3189 - val_loss: 1.3070\n",
            "Epoch 67/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.2867 - val_loss: 1.2708\n",
            "Epoch 68/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.2616 - val_loss: 1.2546\n",
            "Epoch 69/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.2343 - val_loss: 1.2369\n",
            "Epoch 70/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.2226 - val_loss: 1.2197\n",
            "Epoch 71/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.2220 - val_loss: 1.2284\n",
            "Epoch 72/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.2246 - val_loss: 1.2211\n",
            "Epoch 73/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.2218 - val_loss: 1.2237\n",
            "Epoch 74/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.2077 - val_loss: 1.2017\n",
            "Epoch 75/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.1843 - val_loss: 1.1797\n",
            "Epoch 76/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.1690 - val_loss: 1.1683\n",
            "Epoch 77/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.1548 - val_loss: 1.1469\n",
            "Epoch 78/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.1398 - val_loss: 1.1366\n",
            "Epoch 79/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.1283 - val_loss: 1.1250\n",
            "Epoch 80/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.1252 - val_loss: 1.1191\n",
            "Epoch 81/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.1181 - val_loss: 1.1143\n",
            "Epoch 82/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.1088 - val_loss: 1.1032\n",
            "Epoch 83/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.1048 - val_loss: 1.1061\n",
            "Epoch 84/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.1009 - val_loss: 1.0958\n",
            "Epoch 85/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.0918 - val_loss: 1.0889\n",
            "Epoch 86/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.0937 - val_loss: 1.0896\n",
            "Epoch 87/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.0796 - val_loss: 1.0832\n",
            "Epoch 88/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.0811 - val_loss: 1.1044\n",
            "Epoch 89/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.0900 - val_loss: 1.1033\n",
            "Epoch 90/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.0942 - val_loss: 1.0961\n",
            "Epoch 91/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.0830 - val_loss: 1.0747\n",
            "Epoch 92/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.0679 - val_loss: 1.0725\n",
            "Epoch 93/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.0648 - val_loss: 1.0700\n",
            "Epoch 94/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.0901 - val_loss: 1.1249\n",
            "Epoch 95/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.1135 - val_loss: 1.1445\n",
            "Epoch 96/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.1375 - val_loss: 1.1376\n",
            "Epoch 97/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.1247 - val_loss: 1.1550\n",
            "Epoch 98/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.1348 - val_loss: 1.2068\n",
            "Epoch 99/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.2625 - val_loss: 1.2719\n",
            "Epoch 100/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.3118 - val_loss: 1.3683\n",
            "Epoch 101/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.4336 - val_loss: 1.4368\n",
            "Epoch 102/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.4240 - val_loss: 1.4125\n",
            "Epoch 103/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.3924 - val_loss: 1.3801\n",
            "Epoch 104/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.3645 - val_loss: 1.3722\n",
            "Epoch 105/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.3374 - val_loss: 1.3176\n",
            "Epoch 106/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.3032 - val_loss: 1.2905\n",
            "Epoch 107/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.2894 - val_loss: 1.2825\n",
            "Epoch 108/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.2597 - val_loss: 1.2560\n",
            "Epoch 109/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.2419 - val_loss: 1.2362\n",
            "Epoch 110/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.2273 - val_loss: 1.2135\n",
            "Epoch 111/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.2053 - val_loss: 1.2015\n",
            "Epoch 112/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.1958 - val_loss: 1.1866\n",
            "Epoch 113/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.1760 - val_loss: 1.1624\n",
            "Epoch 114/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.1703 - val_loss: 1.1680\n",
            "Epoch 115/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.1641 - val_loss: 1.1638\n",
            "Epoch 116/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.1557 - val_loss: 1.1667\n",
            "Epoch 117/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.1378 - val_loss: 1.1453\n",
            "Epoch 118/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.1247 - val_loss: 1.1303\n",
            "Epoch 119/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.1414 - val_loss: 1.2039\n",
            "Epoch 120/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.1877 - val_loss: 1.1913\n",
            "Epoch 121/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.1848 - val_loss: 1.1298\n",
            "Epoch 122/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.1684 - val_loss: 1.1635\n",
            "Epoch 123/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.1582 - val_loss: 1.1457\n",
            "Epoch 124/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.1766 - val_loss: 1.1953\n",
            "Epoch 125/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.1350 - val_loss: 1.2071\n",
            "Epoch 126/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.1772 - val_loss: 1.1784\n",
            "Epoch 127/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.1632 - val_loss: 1.1551\n",
            "Epoch 128/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.1518 - val_loss: 0.6180\n",
            "Epoch 129/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.3096 - val_loss: 1.3601\n",
            "Epoch 130/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.3333 - val_loss: 1.3504\n",
            "Epoch 131/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.3498 - val_loss: 1.3538\n",
            "Epoch 132/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.3289 - val_loss: 1.3259\n",
            "Epoch 133/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.3110 - val_loss: 1.2934\n",
            "Epoch 134/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.2834 - val_loss: 1.3004\n",
            "Epoch 135/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.3011 - val_loss: 1.2948\n",
            "Epoch 136/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.3707 - val_loss: 1.5224\n",
            "Epoch 137/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.6123 - val_loss: 1.6974\n",
            "Epoch 138/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.7220 - val_loss: 1.7055\n",
            "Epoch 139/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.6731 - val_loss: 1.6436\n",
            "Epoch 140/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.6342 - val_loss: 1.6108\n",
            "Epoch 141/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.5734 - val_loss: 1.5377\n",
            "Epoch 142/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.5317 - val_loss: 1.5423\n",
            "Epoch 143/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.5352 - val_loss: 1.5478\n",
            "Epoch 144/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.5261 - val_loss: 1.5086\n",
            "Epoch 145/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.4980 - val_loss: 1.4884\n",
            "Epoch 146/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.4731 - val_loss: 1.4554\n",
            "Epoch 147/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.4365 - val_loss: 1.4251\n",
            "Epoch 148/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.4020 - val_loss: 1.3844\n",
            "Epoch 149/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.3751 - val_loss: 1.3637\n",
            "Epoch 150/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.3518 - val_loss: 1.3109\n",
            "Epoch 151/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.5032 - val_loss: 1.6283\n",
            "Epoch 152/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.6193 - val_loss: 1.6246\n",
            "Epoch 153/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.6069 - val_loss: 1.6161\n",
            "Epoch 154/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.6238 - val_loss: 1.6048\n",
            "Epoch 155/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.6060 - val_loss: 1.5941\n",
            "Epoch 156/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.5956 - val_loss: 1.5965\n",
            "Epoch 157/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.5776 - val_loss: 1.5648\n",
            "Epoch 158/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.5488 - val_loss: 1.5399\n",
            "Epoch 159/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.5286 - val_loss: 1.5271\n",
            "Epoch 160/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.5224 - val_loss: 1.5217\n",
            "Epoch 161/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.5187 - val_loss: 1.5206\n",
            "Epoch 162/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.5052 - val_loss: 1.5050\n",
            "Epoch 163/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.5003 - val_loss: 1.5426\n",
            "Epoch 164/1000\n",
            "82/82 [==============================] - 55s 673ms/step - loss: 1.5467 - val_loss: 1.5618\n",
            "Epoch 165/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.5619 - val_loss: 1.5948\n",
            "Epoch 166/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.6833 - val_loss: 1.7005\n",
            "Epoch 167/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.6850 - val_loss: 1.6684\n",
            "Epoch 168/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.6563 - val_loss: 1.6279\n",
            "Epoch 169/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.6248 - val_loss: 1.6014\n",
            "Epoch 170/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.5921 - val_loss: 1.5831\n",
            "Epoch 171/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.5616 - val_loss: 1.5500\n",
            "Epoch 172/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.5332 - val_loss: 1.5276\n",
            "Epoch 173/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.5128 - val_loss: 1.5098\n",
            "Epoch 174/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.4945 - val_loss: 1.4836\n",
            "Epoch 175/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.4739 - val_loss: 1.4743\n",
            "Epoch 176/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.4632 - val_loss: 1.4458\n",
            "Epoch 177/1000\n",
            "82/82 [==============================] - 55s 669ms/step - loss: 1.4376 - val_loss: 1.4264\n",
            "Epoch 178/1000\n",
            "82/82 [==============================] - 55s 670ms/step - loss: 1.4169 - val_loss: 1.4121\n",
            "Epoch 00178: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxBguKqoIDme",
        "outputId": "0622aa19-b1f9-4bbb-c1ac-933600171dc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "# loss 그래프\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(\"val_graph\")\n",
        "plt.plot(history.history['loss'], 'y', label='loss')\n",
        "plt.plot(history.history['val_loss'],'b', label='val_loss')\n",
        "\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8gCp29iMgVS"
      },
      "source": [
        "model.save('/content/drive/My Drive/기업프로젝트-라젠/model/resnet_50-2.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHk-RPdHIHMb",
        "outputId": "148ac1ed-1d6c-43e7-fae9-aa535891d3ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# resnet_50\n",
        "hub_url = \"https://tfhub.dev/tensorflow/resnet_50/classification/1\"\n",
        "hub_layer = hub.KerasLayer(hub_url,\n",
        "                           input_shape=IMAGE_SIZE + (3,),\n",
        "                           trainable=True)\n",
        "\n",
        "# model구축\n",
        "# triplets 128벡터값을 위한 dense출력값을 128로 설정, activation = None\n",
        "# deep architecture이후 l2정규화\n",
        "model = Sequential()\n",
        "model.add(hub_layer)\n",
        "model.add(Dense(128, activation=None))\n",
        "model.add(Lambda(lambda x: tf.math.l2_normalize(x, axis=1)))  \n",
        "\n",
        "model.summary()\n",
        "\n",
        "# 모델 complie\n",
        "adam = Adam(lr=0.001, decay=1e-6)\n",
        "model.compile(optimizer=adam, loss=tfa.losses.TripletSemiHardLoss())\n",
        "\n",
        "# SGD = SGD(lr=0.05)\n",
        "# model.compile(optimizer=SGD, loss=tfa.losses.TripletSemiHardLoss())\n",
        "\n",
        "# Adagrad = Adagrad(lr=0.01, epsilon=1e-6)\n",
        "# model.compile(optimizer=adam, loss=tfa.losses.TripletSemiHardLoss())\n",
        "\n",
        "# earlystopping\n",
        "# early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)\n",
        "\n",
        "# 모델 학습\n",
        "history = model.fit(train_ds,\n",
        "                    validation_data = val_ds,\n",
        "                    epochs=1000,\n",
        "                    # callbacks=[early_stopping]\n",
        "                    )  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "keras_layer_2 (KerasLayer)   (None, 1001)              25612201  \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 128)               128256    \n",
            "_________________________________________________________________\n",
            "lambda_2 (Lambda)            (None, 128)               0         \n",
            "=================================================================\n",
            "Total params: 25,740,457\n",
            "Trainable params: 25,687,337\n",
            "Non-trainable params: 53,120\n",
            "_________________________________________________________________\n",
            "Epoch 1/1000\n",
            " 2/82 [..............................] - ETA: 40s - loss: 1.4477WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.2353s vs `on_train_batch_end` time: 0.3940s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.2353s vs `on_train_batch_end` time: 0.3940s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "82/82 [==============================] - 56s 687ms/step - loss: 1.5080 - val_loss: 1.5284\n",
            "Epoch 2/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.5037 - val_loss: 1.4934\n",
            "Epoch 3/1000\n",
            "82/82 [==============================] - 55s 676ms/step - loss: 1.4711 - val_loss: 1.4463\n",
            "Epoch 4/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.4262 - val_loss: 1.3995\n",
            "Epoch 5/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3854 - val_loss: 1.3633\n",
            "Epoch 6/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3612 - val_loss: 1.3547\n",
            "Epoch 7/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3358 - val_loss: 1.3115\n",
            "Epoch 8/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3048 - val_loss: 1.2885\n",
            "Epoch 9/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2809 - val_loss: 1.2688\n",
            "Epoch 10/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2610 - val_loss: 1.2531\n",
            "Epoch 11/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2444 - val_loss: 1.2370\n",
            "Epoch 12/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2306 - val_loss: 1.2254\n",
            "Epoch 13/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2093 - val_loss: 1.2096\n",
            "Epoch 14/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2084 - val_loss: 1.2085\n",
            "Epoch 15/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2320 - val_loss: 1.2892\n",
            "Epoch 16/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2881 - val_loss: 1.3150\n",
            "Epoch 17/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2918 - val_loss: 1.2767\n",
            "Epoch 18/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2726 - val_loss: 1.2603\n",
            "Epoch 19/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2599 - val_loss: 1.2481\n",
            "Epoch 20/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2339 - val_loss: 1.2209\n",
            "Epoch 21/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2257 - val_loss: 1.2124\n",
            "Epoch 22/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2057 - val_loss: 1.1840\n",
            "Epoch 23/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1917 - val_loss: 1.1888\n",
            "Epoch 24/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1756 - val_loss: 1.1555\n",
            "Epoch 25/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1593 - val_loss: 1.1450\n",
            "Epoch 26/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1529 - val_loss: 1.1489\n",
            "Epoch 27/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1598 - val_loss: 1.1670\n",
            "Epoch 28/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1445 - val_loss: 1.1423\n",
            "Epoch 29/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1275 - val_loss: 1.0982\n",
            "Epoch 30/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1261 - val_loss: 1.1278\n",
            "Epoch 31/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1198 - val_loss: 1.0966\n",
            "Epoch 32/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1018 - val_loss: 1.1052\n",
            "Epoch 33/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1020 - val_loss: 1.0723\n",
            "Epoch 34/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0814 - val_loss: 1.0992\n",
            "Epoch 35/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0890 - val_loss: 1.0626\n",
            "Epoch 36/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0837 - val_loss: 1.1064\n",
            "Epoch 37/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0983 - val_loss: 1.1285\n",
            "Epoch 38/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1329 - val_loss: 1.1432\n",
            "Epoch 39/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1380 - val_loss: 1.1487\n",
            "Epoch 40/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1466 - val_loss: 1.1310\n",
            "Epoch 41/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1550 - val_loss: 1.1453\n",
            "Epoch 42/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1163 - val_loss: 1.1167\n",
            "Epoch 43/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1176 - val_loss: 1.1227\n",
            "Epoch 44/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1084 - val_loss: 1.0635\n",
            "Epoch 45/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1053 - val_loss: 1.1382\n",
            "Epoch 46/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1258 - val_loss: 1.1458\n",
            "Epoch 47/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1312 - val_loss: 1.1262\n",
            "Epoch 48/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1181 - val_loss: 1.1320\n",
            "Epoch 49/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1236 - val_loss: 1.1137\n",
            "Epoch 50/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1081 - val_loss: 1.1029\n",
            "Epoch 51/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0971 - val_loss: 1.1009\n",
            "Epoch 52/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0823 - val_loss: 1.0258\n",
            "Epoch 53/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1031 - val_loss: 1.1384\n",
            "Epoch 54/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1696 - val_loss: 1.2415\n",
            "Epoch 55/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2349 - val_loss: 1.3037\n",
            "Epoch 56/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3242 - val_loss: 1.4044\n",
            "Epoch 57/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2202 - val_loss: 1.3454\n",
            "Epoch 58/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3742 - val_loss: 1.3977\n",
            "Epoch 59/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3793 - val_loss: 1.3982\n",
            "Epoch 60/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3893 - val_loss: 1.3857\n",
            "Epoch 61/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3823 - val_loss: 1.3712\n",
            "Epoch 62/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3547 - val_loss: 1.3492\n",
            "Epoch 63/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3384 - val_loss: 1.3132\n",
            "Epoch 64/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3232 - val_loss: 1.3153\n",
            "Epoch 65/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3152 - val_loss: 1.3382\n",
            "Epoch 66/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3363 - val_loss: 1.3399\n",
            "Epoch 67/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3232 - val_loss: 1.3112\n",
            "Epoch 68/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3141 - val_loss: 1.3126\n",
            "Epoch 69/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3080 - val_loss: 1.2958\n",
            "Epoch 70/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2885 - val_loss: 1.2894\n",
            "Epoch 71/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3049 - val_loss: 1.3645\n",
            "Epoch 72/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.4249 - val_loss: 1.4547\n",
            "Epoch 73/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.4364 - val_loss: 1.4221\n",
            "Epoch 74/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.4006 - val_loss: 1.3953\n",
            "Epoch 75/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3738 - val_loss: 1.3570\n",
            "Epoch 76/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3411 - val_loss: 1.3263\n",
            "Epoch 77/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3349 - val_loss: 1.3329\n",
            "Epoch 78/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3237 - val_loss: 1.3122\n",
            "Epoch 79/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3038 - val_loss: 1.3227\n",
            "Epoch 80/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3349 - val_loss: 1.3508\n",
            "Epoch 81/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3496 - val_loss: 1.3354\n",
            "Epoch 82/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3466 - val_loss: 1.3852\n",
            "Epoch 83/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.4618 - val_loss: 1.5422\n",
            "Epoch 84/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.5567 - val_loss: 1.5503\n",
            "Epoch 85/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.5300 - val_loss: 1.5104\n",
            "Epoch 86/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.5011 - val_loss: 1.4975\n",
            "Epoch 87/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.4929 - val_loss: 1.4827\n",
            "Epoch 88/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.4831 - val_loss: 1.4991\n",
            "Epoch 89/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.5068 - val_loss: 1.4862\n",
            "Epoch 90/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.4729 - val_loss: 1.4467\n",
            "Epoch 91/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.4361 - val_loss: 1.4380\n",
            "Epoch 92/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.4287 - val_loss: 1.4565\n",
            "Epoch 93/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.4564 - val_loss: 1.4664\n",
            "Epoch 94/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.4616 - val_loss: 1.4535\n",
            "Epoch 95/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.4249 - val_loss: 1.4095\n",
            "Epoch 96/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3932 - val_loss: 1.3636\n",
            "Epoch 97/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3683 - val_loss: 1.3898\n",
            "Epoch 98/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3667 - val_loss: 1.3551\n",
            "Epoch 99/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3468 - val_loss: 1.3390\n",
            "Epoch 100/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3273 - val_loss: 1.3148\n",
            "Epoch 101/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3069 - val_loss: 1.3107\n",
            "Epoch 102/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3030 - val_loss: 1.2910\n",
            "Epoch 103/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2872 - val_loss: 1.2835\n",
            "Epoch 104/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2730 - val_loss: 1.2626\n",
            "Epoch 105/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2515 - val_loss: 1.2335\n",
            "Epoch 106/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2432 - val_loss: 1.2605\n",
            "Epoch 107/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2504 - val_loss: 1.2510\n",
            "Epoch 108/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2958 - val_loss: 1.3233\n",
            "Epoch 109/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3950 - val_loss: 1.4615\n",
            "Epoch 110/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.4411 - val_loss: 1.4307\n",
            "Epoch 111/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.4193 - val_loss: 1.4158\n",
            "Epoch 112/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3920 - val_loss: 1.3713\n",
            "Epoch 113/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3552 - val_loss: 1.3335\n",
            "Epoch 114/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3294 - val_loss: 1.3247\n",
            "Epoch 115/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3220 - val_loss: 1.3334\n",
            "Epoch 116/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3231 - val_loss: 1.3036\n",
            "Epoch 117/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3025 - val_loss: 1.2884\n",
            "Epoch 118/1000\n",
            "82/82 [==============================] - 56s 679ms/step - loss: 1.2788 - val_loss: 1.2816\n",
            "Epoch 119/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2628 - val_loss: 1.2582\n",
            "Epoch 120/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2568 - val_loss: 1.2465\n",
            "Epoch 121/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2449 - val_loss: 1.2448\n",
            "Epoch 122/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2351 - val_loss: 1.2246\n",
            "Epoch 123/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2141 - val_loss: 1.1972\n",
            "Epoch 124/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1970 - val_loss: 1.1818\n",
            "Epoch 125/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1792 - val_loss: 1.1767\n",
            "Epoch 126/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1739 - val_loss: 1.1736\n",
            "Epoch 127/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1634 - val_loss: 1.1518\n",
            "Epoch 128/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1497 - val_loss: 1.1498\n",
            "Epoch 129/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1454 - val_loss: 1.1315\n",
            "Epoch 130/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1593 - val_loss: 1.1760\n",
            "Epoch 131/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1754 - val_loss: 1.1727\n",
            "Epoch 132/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2184 - val_loss: 1.2521\n",
            "Epoch 133/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2633 - val_loss: 1.2762\n",
            "Epoch 134/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3140 - val_loss: 1.3176\n",
            "Epoch 135/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3290 - val_loss: 1.3310\n",
            "Epoch 136/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3143 - val_loss: 1.2927\n",
            "Epoch 137/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2945 - val_loss: 1.2896\n",
            "Epoch 138/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2856 - val_loss: 1.2943\n",
            "Epoch 139/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2770 - val_loss: 1.2632\n",
            "Epoch 140/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2662 - val_loss: 1.2612\n",
            "Epoch 141/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2662 - val_loss: 1.2637\n",
            "Epoch 142/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2611 - val_loss: 1.2495\n",
            "Epoch 143/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2389 - val_loss: 1.2263\n",
            "Epoch 144/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2245 - val_loss: 1.2138\n",
            "Epoch 145/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2265 - val_loss: 1.2330\n",
            "Epoch 146/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2378 - val_loss: 1.2476\n",
            "Epoch 147/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2662 - val_loss: 1.3076\n",
            "Epoch 148/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2991 - val_loss: 1.2818\n",
            "Epoch 149/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2845 - val_loss: 1.2778\n",
            "Epoch 150/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2717 - val_loss: 1.2639\n",
            "Epoch 151/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2626 - val_loss: 1.2488\n",
            "Epoch 152/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2459 - val_loss: 1.2459\n",
            "Epoch 153/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2526 - val_loss: 1.2515\n",
            "Epoch 154/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2646 - val_loss: 1.2973\n",
            "Epoch 155/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3039 - val_loss: 1.3041\n",
            "Epoch 156/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3159 - val_loss: 1.3542\n",
            "Epoch 157/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3447 - val_loss: 1.3514\n",
            "Epoch 158/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3501 - val_loss: 1.3894\n",
            "Epoch 159/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3638 - val_loss: 1.3511\n",
            "Epoch 160/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3505 - val_loss: 1.3635\n",
            "Epoch 161/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3745 - val_loss: 1.3714\n",
            "Epoch 162/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3532 - val_loss: 1.3456\n",
            "Epoch 163/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3638 - val_loss: 1.3719\n",
            "Epoch 164/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3612 - val_loss: 1.3577\n",
            "Epoch 165/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3613 - val_loss: 1.4137\n",
            "Epoch 166/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.4058 - val_loss: 1.3946\n",
            "Epoch 167/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3852 - val_loss: 1.3881\n",
            "Epoch 168/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3823 - val_loss: 1.4111\n",
            "Epoch 169/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3984 - val_loss: 1.4061\n",
            "Epoch 170/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.4172 - val_loss: 1.4153\n",
            "Epoch 171/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.4059 - val_loss: 1.3930\n",
            "Epoch 172/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3817 - val_loss: 1.3801\n",
            "Epoch 173/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3783 - val_loss: 1.3868\n",
            "Epoch 174/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3733 - val_loss: 1.3526\n",
            "Epoch 175/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3518 - val_loss: 1.3405\n",
            "Epoch 176/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3320 - val_loss: 1.3289\n",
            "Epoch 177/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3191 - val_loss: 1.3064\n",
            "Epoch 178/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2991 - val_loss: 1.2872\n",
            "Epoch 179/1000\n",
            "82/82 [==============================] - 55s 676ms/step - loss: 1.2813 - val_loss: 1.2808\n",
            "Epoch 180/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2732 - val_loss: 1.2581\n",
            "Epoch 181/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2675 - val_loss: 1.2957\n",
            "Epoch 182/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3001 - val_loss: 1.2747\n",
            "Epoch 183/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2766 - val_loss: 1.2765\n",
            "Epoch 184/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2668 - val_loss: 1.2666\n",
            "Epoch 185/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2771 - val_loss: 1.3302\n",
            "Epoch 186/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3367 - val_loss: 1.3389\n",
            "Epoch 187/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3336 - val_loss: 1.3197\n",
            "Epoch 188/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3164 - val_loss: 1.3129\n",
            "Epoch 189/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3071 - val_loss: 1.3027\n",
            "Epoch 190/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2981 - val_loss: 1.2919\n",
            "Epoch 191/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2952 - val_loss: 1.2791\n",
            "Epoch 192/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2933 - val_loss: 1.2910\n",
            "Epoch 193/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3037 - val_loss: 1.3133\n",
            "Epoch 194/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3046 - val_loss: 1.2950\n",
            "Epoch 195/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2827 - val_loss: 1.2672\n",
            "Epoch 196/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2765 - val_loss: 1.2962\n",
            "Epoch 197/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2939 - val_loss: 1.2921\n",
            "Epoch 198/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2948 - val_loss: 1.2936\n",
            "Epoch 199/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2706 - val_loss: 1.2512\n",
            "Epoch 200/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2427 - val_loss: 1.2433\n",
            "Epoch 201/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2411 - val_loss: 1.2432\n",
            "Epoch 202/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2414 - val_loss: 1.2215\n",
            "Epoch 203/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2302 - val_loss: 1.2390\n",
            "Epoch 204/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2402 - val_loss: 1.2276\n",
            "Epoch 205/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.2203 - val_loss: 1.2112\n",
            "Epoch 206/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2114 - val_loss: 1.2041\n",
            "Epoch 207/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2099 - val_loss: 1.2015\n",
            "Epoch 208/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1995 - val_loss: 1.1940\n",
            "Epoch 209/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1887 - val_loss: 1.1897\n",
            "Epoch 210/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1870 - val_loss: 1.1773\n",
            "Epoch 211/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1766 - val_loss: 1.1685\n",
            "Epoch 212/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1659 - val_loss: 1.1493\n",
            "Epoch 213/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1619 - val_loss: 1.1576\n",
            "Epoch 214/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1605 - val_loss: 1.1421\n",
            "Epoch 215/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1460 - val_loss: 1.1352\n",
            "Epoch 216/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1376 - val_loss: 1.1275\n",
            "Epoch 217/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1313 - val_loss: 1.1193\n",
            "Epoch 218/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1285 - val_loss: 1.1882\n",
            "Epoch 219/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1817 - val_loss: 1.1738\n",
            "Epoch 220/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1734 - val_loss: 1.1577\n",
            "Epoch 221/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1555 - val_loss: 1.1374\n",
            "Epoch 222/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1707 - val_loss: 1.2077\n",
            "Epoch 223/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1947 - val_loss: 1.1820\n",
            "Epoch 224/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1741 - val_loss: 1.1627\n",
            "Epoch 225/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1568 - val_loss: 1.1493\n",
            "Epoch 226/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1488 - val_loss: 1.1556\n",
            "Epoch 227/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1472 - val_loss: 1.1474\n",
            "Epoch 228/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1491 - val_loss: 1.1484\n",
            "Epoch 229/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1510 - val_loss: 1.1525\n",
            "Epoch 230/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1613 - val_loss: 1.1837\n",
            "Epoch 231/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2002 - val_loss: 1.2277\n",
            "Epoch 232/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2179 - val_loss: 1.2050\n",
            "Epoch 233/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1939 - val_loss: 1.1834\n",
            "Epoch 234/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1743 - val_loss: 1.1656\n",
            "Epoch 235/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1580 - val_loss: 1.1506\n",
            "Epoch 236/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1440 - val_loss: 1.1375\n",
            "Epoch 237/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1310 - val_loss: 1.1260\n",
            "Epoch 238/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1207 - val_loss: 1.1145\n",
            "Epoch 239/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1988 - val_loss: 1.2623\n",
            "Epoch 240/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2610 - val_loss: 1.2908\n",
            "Epoch 241/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2747 - val_loss: 1.2554\n",
            "Epoch 242/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2399 - val_loss: 1.2254\n",
            "Epoch 243/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2133 - val_loss: 1.2018\n",
            "Epoch 244/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1918 - val_loss: 1.1822\n",
            "Epoch 245/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1718 - val_loss: 1.1513\n",
            "Epoch 246/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1593 - val_loss: 1.1370\n",
            "Epoch 247/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1568 - val_loss: 1.1719\n",
            "Epoch 248/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1659 - val_loss: 1.1652\n",
            "Epoch 249/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1541 - val_loss: 1.1264\n",
            "Epoch 250/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1530 - val_loss: 1.1705\n",
            "Epoch 251/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1667 - val_loss: 1.1552\n",
            "Epoch 252/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1460 - val_loss: 1.1373\n",
            "Epoch 253/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1482 - val_loss: 1.1553\n",
            "Epoch 254/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1441 - val_loss: 1.1340\n",
            "Epoch 255/1000\n",
            "82/82 [==============================] - 55s 676ms/step - loss: 1.1260 - val_loss: 1.1185\n",
            "Epoch 256/1000\n",
            "82/82 [==============================] - 55s 676ms/step - loss: 1.1288 - val_loss: 1.1693\n",
            "Epoch 257/1000\n",
            "82/82 [==============================] - 55s 676ms/step - loss: 1.1630 - val_loss: 1.1520\n",
            "Epoch 258/1000\n",
            "82/82 [==============================] - 55s 676ms/step - loss: 1.1609 - val_loss: 1.1561\n",
            "Epoch 259/1000\n",
            "82/82 [==============================] - 55s 676ms/step - loss: 1.1760 - val_loss: 1.1954\n",
            "Epoch 260/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2682 - val_loss: 1.3513\n",
            "Epoch 261/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3388 - val_loss: 1.3186\n",
            "Epoch 262/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3004 - val_loss: 1.2826\n",
            "Epoch 263/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2674 - val_loss: 1.2529\n",
            "Epoch 264/1000\n",
            "82/82 [==============================] - 55s 676ms/step - loss: 1.2403 - val_loss: 1.2282\n",
            "Epoch 265/1000\n",
            "82/82 [==============================] - 55s 676ms/step - loss: 1.2176 - val_loss: 1.2072\n",
            "Epoch 266/1000\n",
            "82/82 [==============================] - 55s 676ms/step - loss: 1.1980 - val_loss: 1.1891\n",
            "Epoch 267/1000\n",
            "82/82 [==============================] - 55s 676ms/step - loss: 1.1819 - val_loss: 1.1765\n",
            "Epoch 268/1000\n",
            "82/82 [==============================] - 55s 676ms/step - loss: 1.1688 - val_loss: 1.1613\n",
            "Epoch 269/1000\n",
            "82/82 [==============================] - 55s 676ms/step - loss: 1.1547 - val_loss: 1.1482\n",
            "Epoch 270/1000\n",
            "82/82 [==============================] - 55s 676ms/step - loss: 1.1424 - val_loss: 1.1366\n",
            "Epoch 271/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1314 - val_loss: 1.1263\n",
            "Epoch 272/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1216 - val_loss: 1.1170\n",
            "Epoch 273/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1128 - val_loss: 1.1086\n",
            "Epoch 274/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1048 - val_loss: 1.1009\n",
            "Epoch 275/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0974 - val_loss: 1.0939\n",
            "Epoch 276/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0907 - val_loss: 1.0875\n",
            "Epoch 277/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0846 - val_loss: 1.0816\n",
            "Epoch 278/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0789 - val_loss: 1.0762\n",
            "Epoch 279/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0737 - val_loss: 1.0712\n",
            "Epoch 280/1000\n",
            "82/82 [==============================] - 55s 676ms/step - loss: 1.0690 - val_loss: 1.0667\n",
            "Epoch 281/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0645 - val_loss: 1.0622\n",
            "Epoch 282/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1063 - val_loss: 1.2182\n",
            "Epoch 283/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2841 - val_loss: 1.3496\n",
            "Epoch 284/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3640 - val_loss: 1.3404\n",
            "Epoch 285/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3097 - val_loss: 1.2996\n",
            "Epoch 286/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2776 - val_loss: 1.2677\n",
            "Epoch 287/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2540 - val_loss: 1.2417\n",
            "Epoch 288/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2318 - val_loss: 1.2215\n",
            "Epoch 289/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2118 - val_loss: 1.2022\n",
            "Epoch 290/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1937 - val_loss: 1.1855\n",
            "Epoch 291/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1782 - val_loss: 1.1709\n",
            "Epoch 292/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1645 - val_loss: 1.1580\n",
            "Epoch 293/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1522 - val_loss: 1.1465\n",
            "Epoch 294/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1413 - val_loss: 1.1361\n",
            "Epoch 295/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1313 - val_loss: 1.1266\n",
            "Epoch 296/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1222 - val_loss: 1.1179\n",
            "Epoch 297/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1139 - val_loss: 1.1101\n",
            "Epoch 298/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1064 - val_loss: 1.1027\n",
            "Epoch 299/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0993 - val_loss: 1.0959\n",
            "Epoch 300/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0927 - val_loss: 1.0896\n",
            "Epoch 301/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0867 - val_loss: 1.0837\n",
            "Epoch 302/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0810 - val_loss: 1.0783\n",
            "Epoch 303/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0756 - val_loss: 1.0763\n",
            "Epoch 304/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2104 - val_loss: 1.3226\n",
            "Epoch 305/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3201 - val_loss: 1.3016\n",
            "Epoch 306/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2906 - val_loss: 1.2795\n",
            "Epoch 307/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2773 - val_loss: 1.2974\n",
            "Epoch 308/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3345 - val_loss: 1.4149\n",
            "Epoch 309/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.4408 - val_loss: 1.4003\n",
            "Epoch 310/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.4316 - val_loss: 1.4294\n",
            "Epoch 311/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.4149 - val_loss: 1.4010\n",
            "Epoch 312/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3832 - val_loss: 1.3710\n",
            "Epoch 313/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3707 - val_loss: 1.3721\n",
            "Epoch 314/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3568 - val_loss: 1.3421\n",
            "Epoch 315/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3293 - val_loss: 1.3167\n",
            "Epoch 316/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.3053 - val_loss: 1.2941\n",
            "Epoch 317/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2839 - val_loss: 1.2738\n",
            "Epoch 318/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2645 - val_loss: 1.2553\n",
            "Epoch 319/1000\n",
            "82/82 [==============================] - 56s 677ms/step - loss: 1.2467 - val_loss: 1.2382\n",
            "Epoch 320/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2303 - val_loss: 1.2225\n",
            "Epoch 321/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2151 - val_loss: 1.2078\n",
            "Epoch 322/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2011 - val_loss: 1.1943\n",
            "Epoch 323/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1879 - val_loss: 1.1816\n",
            "Epoch 324/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1757 - val_loss: 1.1697\n",
            "Epoch 325/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1642 - val_loss: 1.1586\n",
            "Epoch 326/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1535 - val_loss: 1.1483\n",
            "Epoch 327/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1579 - val_loss: 1.1799\n",
            "Epoch 328/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1869 - val_loss: 1.1815\n",
            "Epoch 329/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1875 - val_loss: 1.1764\n",
            "Epoch 330/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1856 - val_loss: 1.1915\n",
            "Epoch 331/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1864 - val_loss: 1.1674\n",
            "Epoch 332/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1709 - val_loss: 1.1882\n",
            "Epoch 333/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1783 - val_loss: 1.1686\n",
            "Epoch 334/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1603 - val_loss: 1.1524\n",
            "Epoch 335/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1454 - val_loss: 1.1386\n",
            "Epoch 336/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1325 - val_loss: 1.1265\n",
            "Epoch 337/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1211 - val_loss: 1.1158\n",
            "Epoch 338/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1109 - val_loss: 1.1062\n",
            "Epoch 339/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1018 - val_loss: 1.0975\n",
            "Epoch 340/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0935 - val_loss: 1.0896\n",
            "Epoch 341/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0860 - val_loss: 1.0824\n",
            "Epoch 342/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0791 - val_loss: 1.0758\n",
            "Epoch 343/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0728 - val_loss: 1.0698\n",
            "Epoch 344/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0671 - val_loss: 1.0643\n",
            "Epoch 345/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0617 - val_loss: 1.0592\n",
            "Epoch 346/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0569 - val_loss: 1.0545\n",
            "Epoch 347/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0734 - val_loss: 1.0788\n",
            "Epoch 348/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0719 - val_loss: 1.0658\n",
            "Epoch 349/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0612 - val_loss: 1.0569\n",
            "Epoch 350/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0535 - val_loss: 1.0502\n",
            "Epoch 351/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0473 - val_loss: 1.0446\n",
            "Epoch 352/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0423 - val_loss: 1.0400\n",
            "Epoch 353/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0380 - val_loss: 1.0360\n",
            "Epoch 354/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0321 - val_loss: 1.0553\n",
            "Epoch 355/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1411 - val_loss: 1.1595\n",
            "Epoch 356/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1726 - val_loss: 1.1677\n",
            "Epoch 357/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1606 - val_loss: 1.1495\n",
            "Epoch 358/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1680 - val_loss: 1.1796\n",
            "Epoch 359/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2111 - val_loss: 1.2331\n",
            "Epoch 360/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2147 - val_loss: 1.1961\n",
            "Epoch 361/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1815 - val_loss: 1.1678\n",
            "Epoch 362/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1566 - val_loss: 1.1450\n",
            "Epoch 363/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1367 - val_loss: 1.1268\n",
            "Epoch 364/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1196 - val_loss: 1.1078\n",
            "Epoch 365/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1087 - val_loss: 1.1018\n",
            "Epoch 366/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0959 - val_loss: 1.0902\n",
            "Epoch 367/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0853 - val_loss: 1.0804\n",
            "Epoch 368/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0761 - val_loss: 1.0720\n",
            "Epoch 369/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0682 - val_loss: 1.0645\n",
            "Epoch 370/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0612 - val_loss: 1.0580\n",
            "Epoch 371/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0551 - val_loss: 1.0522\n",
            "Epoch 372/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0496 - val_loss: 1.0470\n",
            "Epoch 373/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0448 - val_loss: 1.0425\n",
            "Epoch 374/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0405 - val_loss: 1.0385\n",
            "Epoch 375/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0366 - val_loss: 1.0348\n",
            "Epoch 376/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0331 - val_loss: 1.0312\n",
            "Epoch 377/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0441 - val_loss: 1.0443\n",
            "Epoch 378/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0390 - val_loss: 1.0347\n",
            "Epoch 379/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0317 - val_loss: 1.0290\n",
            "Epoch 380/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0269 - val_loss: 1.0250\n",
            "Epoch 381/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0234 - val_loss: 1.0219\n",
            "Epoch 382/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0206 - val_loss: 1.0194\n",
            "Epoch 383/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0183 - val_loss: 1.0173\n",
            "Epoch 384/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0164 - val_loss: 1.0155\n",
            "Epoch 385/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0147 - val_loss: 1.0140\n",
            "Epoch 386/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0133 - val_loss: 1.0126\n",
            "Epoch 387/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0120 - val_loss: 1.0114\n",
            "Epoch 388/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0109 - val_loss: 1.0103\n",
            "Epoch 389/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0099 - val_loss: 1.0094\n",
            "Epoch 390/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0090 - val_loss: 1.0085\n",
            "Epoch 391/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0081 - val_loss: 1.0078\n",
            "Epoch 392/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0076 - val_loss: 1.0913\n",
            "Epoch 393/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2211 - val_loss: 1.2374\n",
            "Epoch 394/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2125 - val_loss: 1.1910\n",
            "Epoch 395/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1750 - val_loss: 1.1602\n",
            "Epoch 396/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1483 - val_loss: 1.1371\n",
            "Epoch 397/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1278 - val_loss: 1.1189\n",
            "Epoch 398/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1113 - val_loss: 1.1040\n",
            "Epoch 399/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0978 - val_loss: 1.0917\n",
            "Epoch 400/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0864 - val_loss: 1.0813\n",
            "Epoch 401/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0768 - val_loss: 1.0725\n",
            "Epoch 402/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0686 - val_loss: 1.0648\n",
            "Epoch 403/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0606 - val_loss: 1.0620\n",
            "Epoch 404/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0582 - val_loss: 1.0546\n",
            "Epoch 405/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0517 - val_loss: 1.0488\n",
            "Epoch 406/1000\n",
            "82/82 [==============================] - 55s 676ms/step - loss: 1.0463 - val_loss: 1.0439\n",
            "Epoch 407/1000\n",
            "82/82 [==============================] - 55s 676ms/step - loss: 1.0521 - val_loss: 1.0681\n",
            "Epoch 408/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0602 - val_loss: 1.0537\n",
            "Epoch 409/1000\n",
            "82/82 [==============================] - 55s 676ms/step - loss: 1.0493 - val_loss: 1.0454\n",
            "Epoch 410/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0423 - val_loss: 1.0394\n",
            "Epoch 411/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0370 - val_loss: 1.0348\n",
            "Epoch 412/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0328 - val_loss: 1.0309\n",
            "Epoch 413/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0293 - val_loss: 1.0277\n",
            "Epoch 414/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0263 - val_loss: 1.0249\n",
            "Epoch 415/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0237 - val_loss: 1.0224\n",
            "Epoch 416/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0214 - val_loss: 1.0203\n",
            "Epoch 417/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0194 - val_loss: 1.0184\n",
            "Epoch 418/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0176 - val_loss: 1.0168\n",
            "Epoch 419/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0160 - val_loss: 1.0164\n",
            "Epoch 420/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0872 - val_loss: 1.1502\n",
            "Epoch 421/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1272 - val_loss: 1.1325\n",
            "Epoch 422/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1113 - val_loss: 1.0903\n",
            "Epoch 423/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1049 - val_loss: 1.1187\n",
            "Epoch 424/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1202 - val_loss: 1.1288\n",
            "Epoch 425/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1546 - val_loss: 1.1214\n",
            "Epoch 426/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1537 - val_loss: 1.1593\n",
            "Epoch 427/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1726 - val_loss: 1.2114\n",
            "Epoch 428/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1848 - val_loss: 1.1839\n",
            "Epoch 429/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1758 - val_loss: 1.1717\n",
            "Epoch 430/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1619 - val_loss: 1.1474\n",
            "Epoch 431/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1607 - val_loss: 1.1534\n",
            "Epoch 432/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1602 - val_loss: 1.1840\n",
            "Epoch 433/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1986 - val_loss: 1.1934\n",
            "Epoch 434/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1859 - val_loss: 1.1902\n",
            "Epoch 435/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1742 - val_loss: 1.1614\n",
            "Epoch 436/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1708 - val_loss: 1.1626\n",
            "Epoch 437/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1592 - val_loss: 1.1575\n",
            "Epoch 438/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1494 - val_loss: 1.1408\n",
            "Epoch 439/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1393 - val_loss: 1.1246\n",
            "Epoch 440/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1285 - val_loss: 1.1201\n",
            "Epoch 441/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1460 - val_loss: 1.1384\n",
            "Epoch 442/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1318 - val_loss: 1.1256\n",
            "Epoch 443/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1202 - val_loss: 1.1150\n",
            "Epoch 444/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1104 - val_loss: 1.1059\n",
            "Epoch 445/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1018 - val_loss: 1.0978\n",
            "Epoch 446/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0942 - val_loss: 1.0907\n",
            "Epoch 447/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0875 - val_loss: 1.0843\n",
            "Epoch 448/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0813 - val_loss: 1.0784\n",
            "Epoch 449/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0757 - val_loss: 1.0731\n",
            "Epoch 450/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0707 - val_loss: 1.0683\n",
            "Epoch 451/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0660 - val_loss: 1.0637\n",
            "Epoch 452/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.0616 - val_loss: 1.0595\n",
            "Epoch 453/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0576 - val_loss: 1.0557\n",
            "Epoch 454/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0776 - val_loss: 1.1715\n",
            "Epoch 455/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1820 - val_loss: 1.1665\n",
            "Epoch 456/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1544 - val_loss: 1.1448\n",
            "Epoch 457/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1604 - val_loss: 1.1735\n",
            "Epoch 458/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1602 - val_loss: 1.1479\n",
            "Epoch 459/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1382 - val_loss: 1.1289\n",
            "Epoch 460/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1212 - val_loss: 1.1138\n",
            "Epoch 461/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1074 - val_loss: 1.1013\n",
            "Epoch 462/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.0959 - val_loss: 1.0908\n",
            "Epoch 463/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0862 - val_loss: 1.0817\n",
            "Epoch 464/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0778 - val_loss: 1.0779\n",
            "Epoch 465/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0778 - val_loss: 1.0776\n",
            "Epoch 466/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0720 - val_loss: 1.0670\n",
            "Epoch 467/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0632 - val_loss: 1.0596\n",
            "Epoch 468/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0566 - val_loss: 1.0537\n",
            "Epoch 469/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0512 - val_loss: 1.0487\n",
            "Epoch 470/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0465 - val_loss: 1.0444\n",
            "Epoch 471/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0425 - val_loss: 1.0406\n",
            "Epoch 472/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0389 - val_loss: 1.0372\n",
            "Epoch 473/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0357 - val_loss: 1.0341\n",
            "Epoch 474/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0328 - val_loss: 1.0314\n",
            "Epoch 475/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0301 - val_loss: 1.0289\n",
            "Epoch 476/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0278 - val_loss: 1.0267\n",
            "Epoch 477/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0256 - val_loss: 1.0246\n",
            "Epoch 478/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0237 - val_loss: 1.0227\n",
            "Epoch 479/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0219 - val_loss: 1.0210\n",
            "Epoch 480/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0202 - val_loss: 1.0194\n",
            "Epoch 481/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0187 - val_loss: 1.0180\n",
            "Epoch 482/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0173 - val_loss: 1.0167\n",
            "Epoch 483/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0781 - val_loss: 1.0903\n",
            "Epoch 484/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0787 - val_loss: 1.0688\n",
            "Epoch 485/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0617 - val_loss: 1.0553\n",
            "Epoch 486/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0504 - val_loss: 1.0458\n",
            "Epoch 487/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0421 - val_loss: 1.0386\n",
            "Epoch 488/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0358 - val_loss: 1.0331\n",
            "Epoch 489/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0308 - val_loss: 1.0286\n",
            "Epoch 490/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0267 - val_loss: 1.0249\n",
            "Epoch 491/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0234 - val_loss: 1.0219\n",
            "Epoch 492/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1476 - val_loss: 1.2326\n",
            "Epoch 493/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.2200 - val_loss: 1.2265\n",
            "Epoch 494/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1962 - val_loss: 1.2019\n",
            "Epoch 495/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1871 - val_loss: 1.1731\n",
            "Epoch 496/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1616 - val_loss: 1.1506\n",
            "Epoch 497/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1412 - val_loss: 1.1323\n",
            "Epoch 498/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1245 - val_loss: 1.1170\n",
            "Epoch 499/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1105 - val_loss: 1.1042\n",
            "Epoch 500/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0987 - val_loss: 1.0932\n",
            "Epoch 501/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0885 - val_loss: 1.0838\n",
            "Epoch 502/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0796 - val_loss: 1.0756\n",
            "Epoch 503/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0719 - val_loss: 1.0684\n",
            "Epoch 504/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0652 - val_loss: 1.0620\n",
            "Epoch 505/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0592 - val_loss: 1.0564\n",
            "Epoch 506/1000\n",
            "82/82 [==============================] - 55s 676ms/step - loss: 1.0539 - val_loss: 1.0515\n",
            "Epoch 507/1000\n",
            "82/82 [==============================] - 55s 676ms/step - loss: 1.0492 - val_loss: 1.0470\n",
            "Epoch 508/1000\n",
            "82/82 [==============================] - 55s 676ms/step - loss: 1.0450 - val_loss: 1.0431\n",
            "Epoch 509/1000\n",
            "82/82 [==============================] - 55s 676ms/step - loss: 1.0413 - val_loss: 1.0394\n",
            "Epoch 510/1000\n",
            "82/82 [==============================] - 55s 676ms/step - loss: 1.0395 - val_loss: 1.0382\n",
            "Epoch 511/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0361 - val_loss: 1.0350\n",
            "Epoch 512/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0375 - val_loss: 1.0354\n",
            "Epoch 513/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0332 - val_loss: 1.0312\n",
            "Epoch 514/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0297 - val_loss: 1.0282\n",
            "Epoch 515/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0269 - val_loss: 1.0257\n",
            "Epoch 516/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0246 - val_loss: 1.0235\n",
            "Epoch 517/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0226 - val_loss: 1.0217\n",
            "Epoch 518/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0209 - val_loss: 1.0201\n",
            "Epoch 519/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0193 - val_loss: 1.0193\n",
            "Epoch 520/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1181 - val_loss: 1.1400\n",
            "Epoch 521/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1301 - val_loss: 1.1178\n",
            "Epoch 522/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1214 - val_loss: 1.1258\n",
            "Epoch 523/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1209 - val_loss: 1.1390\n",
            "Epoch 524/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1041 - val_loss: 1.0898\n",
            "Epoch 525/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0888 - val_loss: 1.0840\n",
            "Epoch 526/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0826 - val_loss: 1.0978\n",
            "Epoch 527/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.0839 - val_loss: 1.0723\n",
            "Epoch 528/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.0810 - val_loss: 1.0817\n",
            "Epoch 529/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0826 - val_loss: 1.0784\n",
            "Epoch 530/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0750 - val_loss: 1.0700\n",
            "Epoch 531/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0664 - val_loss: 1.0569\n",
            "Epoch 532/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0548 - val_loss: 1.0417\n",
            "Epoch 533/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0566 - val_loss: 1.0490\n",
            "Epoch 534/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0502 - val_loss: 1.0416\n",
            "Epoch 535/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.0465 - val_loss: 1.0554\n",
            "Epoch 536/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.0620 - val_loss: 1.0638\n",
            "Epoch 537/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0606 - val_loss: 1.0564\n",
            "Epoch 538/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.0658 - val_loss: 1.0552\n",
            "Epoch 539/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.0758 - val_loss: 1.0861\n",
            "Epoch 540/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1023 - val_loss: 1.1273\n",
            "Epoch 541/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1028 - val_loss: 1.1184\n",
            "Epoch 542/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0986 - val_loss: 1.0990\n",
            "Epoch 543/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1205 - val_loss: 1.1210\n",
            "Epoch 544/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1214 - val_loss: 1.1159\n",
            "Epoch 545/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1049 - val_loss: 1.0882\n",
            "Epoch 546/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0903 - val_loss: 1.0869\n",
            "Epoch 547/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.0732 - val_loss: 1.0653\n",
            "Epoch 548/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0656 - val_loss: 1.0552\n",
            "Epoch 549/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.0577 - val_loss: 1.0487\n",
            "Epoch 550/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.0498 - val_loss: 1.0497\n",
            "Epoch 551/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.0524 - val_loss: 1.0955\n",
            "Epoch 552/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1360 - val_loss: 1.1488\n",
            "Epoch 553/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1406 - val_loss: 1.1297\n",
            "Epoch 554/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1315 - val_loss: 1.1235\n",
            "Epoch 555/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1233 - val_loss: 1.1401\n",
            "Epoch 556/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1169 - val_loss: 1.1003\n",
            "Epoch 557/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1236 - val_loss: 1.1353\n",
            "Epoch 558/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1391 - val_loss: 1.1341\n",
            "Epoch 559/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1215 - val_loss: 1.1070\n",
            "Epoch 560/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.0932 - val_loss: 1.0852\n",
            "Epoch 561/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.0867 - val_loss: 1.0843\n",
            "Epoch 562/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1007 - val_loss: 1.0971\n",
            "Epoch 563/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.0927 - val_loss: 1.0869\n",
            "Epoch 564/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.0814 - val_loss: 1.0762\n",
            "Epoch 565/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.0718 - val_loss: 1.0675\n",
            "Epoch 566/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.0639 - val_loss: 1.0603\n",
            "Epoch 567/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.0572 - val_loss: 1.0542\n",
            "Epoch 568/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.0515 - val_loss: 1.0489\n",
            "Epoch 569/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.0466 - val_loss: 1.0443\n",
            "Epoch 570/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.0423 - val_loss: 1.0402\n",
            "Epoch 571/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.0384 - val_loss: 1.0367\n",
            "Epoch 572/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.0351 - val_loss: 1.0335\n",
            "Epoch 573/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.0346 - val_loss: 1.0330\n",
            "Epoch 574/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.0311 - val_loss: 1.0294\n",
            "Epoch 575/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.0280 - val_loss: 1.0266\n",
            "Epoch 576/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.0254 - val_loss: 1.0243\n",
            "Epoch 577/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.0233 - val_loss: 1.0223\n",
            "Epoch 578/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.0214 - val_loss: 1.0205\n",
            "Epoch 579/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.0197 - val_loss: 1.0189\n",
            "Epoch 580/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.0181 - val_loss: 1.0174\n",
            "Epoch 581/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.0168 - val_loss: 1.0161\n",
            "Epoch 582/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.0155 - val_loss: 1.0149\n",
            "Epoch 583/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.0133 - val_loss: 1.0633\n",
            "Epoch 584/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1419 - val_loss: 1.1384\n",
            "Epoch 585/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1310 - val_loss: 1.1164\n",
            "Epoch 586/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1155 - val_loss: 1.1188\n",
            "Epoch 587/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1160 - val_loss: 1.1313\n",
            "Epoch 588/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1692 - val_loss: 1.1793\n",
            "Epoch 589/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.2026 - val_loss: 1.2382\n",
            "Epoch 590/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.2501 - val_loss: 1.2477\n",
            "Epoch 591/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.2386 - val_loss: 1.2211\n",
            "Epoch 592/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.2125 - val_loss: 1.2132\n",
            "Epoch 593/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.2216 - val_loss: 1.2172\n",
            "Epoch 594/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.2168 - val_loss: 1.2117\n",
            "Epoch 595/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.2005 - val_loss: 1.1909\n",
            "Epoch 596/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1855 - val_loss: 1.2079\n",
            "Epoch 597/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1901 - val_loss: 1.1903\n",
            "Epoch 598/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1731 - val_loss: 1.1673\n",
            "Epoch 599/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1613 - val_loss: 1.1544\n",
            "Epoch 600/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1549 - val_loss: 1.1600\n",
            "Epoch 601/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1479 - val_loss: 1.1359\n",
            "Epoch 602/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1386 - val_loss: 1.1436\n",
            "Epoch 603/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1549 - val_loss: 1.1598\n",
            "Epoch 604/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1529 - val_loss: 1.1497\n",
            "Epoch 605/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1426 - val_loss: 1.1346\n",
            "Epoch 606/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1309 - val_loss: 1.1243\n",
            "Epoch 607/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1285 - val_loss: 1.1238\n",
            "Epoch 608/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1299 - val_loss: 1.1293\n",
            "Epoch 609/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1310 - val_loss: 1.1330\n",
            "Epoch 610/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1247 - val_loss: 1.1336\n",
            "Epoch 611/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1234 - val_loss: 1.1204\n",
            "Epoch 612/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1220 - val_loss: 1.1044\n",
            "Epoch 613/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1094 - val_loss: 1.1096\n",
            "Epoch 614/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1081 - val_loss: 1.1123\n",
            "Epoch 615/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1105 - val_loss: 1.1014\n",
            "Epoch 616/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1058 - val_loss: 1.1020\n",
            "Epoch 617/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.0979 - val_loss: 1.0961\n",
            "Epoch 618/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.0931 - val_loss: 1.0936\n",
            "Epoch 619/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.0918 - val_loss: 1.0934\n",
            "Epoch 620/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.0913 - val_loss: 1.0922\n",
            "Epoch 621/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.0840 - val_loss: 1.0757\n",
            "Epoch 622/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.0814 - val_loss: 1.0795\n",
            "Epoch 623/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.0849 - val_loss: 1.1158\n",
            "Epoch 624/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1115 - val_loss: 1.1131\n",
            "Epoch 625/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.0986 - val_loss: 1.0952\n",
            "Epoch 626/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.0892 - val_loss: 1.0852\n",
            "Epoch 627/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.0950 - val_loss: 1.1154\n",
            "Epoch 628/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1263 - val_loss: 1.1491\n",
            "Epoch 629/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1600 - val_loss: 1.1687\n",
            "Epoch 630/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1821 - val_loss: 1.2073\n",
            "Epoch 631/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.2047 - val_loss: 1.2097\n",
            "Epoch 632/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.2025 - val_loss: 1.1908\n",
            "Epoch 633/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1892 - val_loss: 1.1768\n",
            "Epoch 634/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1842 - val_loss: 1.1833\n",
            "Epoch 635/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1750 - val_loss: 1.1557\n",
            "Epoch 636/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1498 - val_loss: 1.1416\n",
            "Epoch 637/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1412 - val_loss: 1.1393\n",
            "Epoch 638/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1600 - val_loss: 1.1719\n",
            "Epoch 639/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1641 - val_loss: 1.1587\n",
            "Epoch 640/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1509 - val_loss: 1.1510\n",
            "Epoch 641/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1429 - val_loss: 1.1344\n",
            "Epoch 642/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1336 - val_loss: 1.1260\n",
            "Epoch 643/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1233 - val_loss: 1.1194\n",
            "Epoch 644/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1251 - val_loss: 1.1250\n",
            "Epoch 645/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1263 - val_loss: 1.1459\n",
            "Epoch 646/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1381 - val_loss: 1.1306\n",
            "Epoch 647/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1241 - val_loss: 1.1179\n",
            "Epoch 648/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1092 - val_loss: 1.0979\n",
            "Epoch 649/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1355 - val_loss: 1.1634\n",
            "Epoch 650/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1589 - val_loss: 1.1754\n",
            "Epoch 651/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1835 - val_loss: 1.1819\n",
            "Epoch 652/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1751 - val_loss: 1.1796\n",
            "Epoch 653/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1830 - val_loss: 1.1831\n",
            "Epoch 654/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1450 - val_loss: 1.1711\n",
            "Epoch 655/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1992 - val_loss: 1.1931\n",
            "Epoch 656/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1933 - val_loss: 1.1926\n",
            "Epoch 657/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.2088 - val_loss: 1.2027\n",
            "Epoch 658/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1929 - val_loss: 1.1835\n",
            "Epoch 659/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1752 - val_loss: 1.1671\n",
            "Epoch 660/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1598 - val_loss: 1.1527\n",
            "Epoch 661/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1462 - val_loss: 1.1398\n",
            "Epoch 662/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1340 - val_loss: 1.1282\n",
            "Epoch 663/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1229 - val_loss: 1.1177\n",
            "Epoch 664/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1129 - val_loss: 1.1082\n",
            "Epoch 665/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1039 - val_loss: 1.0995\n",
            "Epoch 666/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0956 - val_loss: 1.0917\n",
            "Epoch 667/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0881 - val_loss: 1.0845\n",
            "Epoch 668/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0812 - val_loss: 1.0779\n",
            "Epoch 669/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0749 - val_loss: 1.0719\n",
            "Epoch 670/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0691 - val_loss: 1.0663\n",
            "Epoch 671/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0638 - val_loss: 1.0613\n",
            "Epoch 672/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0613 - val_loss: 1.0658\n",
            "Epoch 673/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0621 - val_loss: 1.0587\n",
            "Epoch 674/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0559 - val_loss: 1.0531\n",
            "Epoch 675/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0508 - val_loss: 1.0484\n",
            "Epoch 676/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0464 - val_loss: 1.0444\n",
            "Epoch 677/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0425 - val_loss: 1.0408\n",
            "Epoch 678/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0391 - val_loss: 1.0375\n",
            "Epoch 679/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.0361 - val_loss: 1.0346\n",
            "Epoch 680/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0333 - val_loss: 1.0320\n",
            "Epoch 681/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0308 - val_loss: 1.0296\n",
            "Epoch 682/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0285 - val_loss: 1.0274\n",
            "Epoch 683/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.0264 - val_loss: 1.0254\n",
            "Epoch 684/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.0245 - val_loss: 1.0236\n",
            "Epoch 685/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.0227 - val_loss: 1.0219\n",
            "Epoch 686/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.0211 - val_loss: 1.0203\n",
            "Epoch 687/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.0196 - val_loss: 1.0189\n",
            "Epoch 688/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.0183 - val_loss: 1.0176\n",
            "Epoch 689/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.0170 - val_loss: 1.0164\n",
            "Epoch 690/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.0686 - val_loss: 1.1511\n",
            "Epoch 691/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1557 - val_loss: 1.1382\n",
            "Epoch 692/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1624 - val_loss: 1.1533\n",
            "Epoch 693/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1450 - val_loss: 1.1435\n",
            "Epoch 694/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1451 - val_loss: 1.1507\n",
            "Epoch 695/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1657 - val_loss: 1.1793\n",
            "Epoch 696/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1679 - val_loss: 1.1673\n",
            "Epoch 697/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1597 - val_loss: 1.1605\n",
            "Epoch 698/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1507 - val_loss: 1.1488\n",
            "Epoch 699/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1433 - val_loss: 1.1271\n",
            "Epoch 700/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1506 - val_loss: 1.2088\n",
            "Epoch 701/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.2144 - val_loss: 1.2041\n",
            "Epoch 702/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.2149 - val_loss: 1.2199\n",
            "Epoch 703/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1923 - val_loss: 1.2232\n",
            "Epoch 704/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1720 - val_loss: 1.2280\n",
            "Epoch 705/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.2139 - val_loss: 1.2255\n",
            "Epoch 706/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.2093 - val_loss: 1.1998\n",
            "Epoch 707/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.2126 - val_loss: 1.2079\n",
            "Epoch 708/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.2246 - val_loss: 1.2296\n",
            "Epoch 709/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.2336 - val_loss: 1.2393\n",
            "Epoch 710/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.2270 - val_loss: 1.2286\n",
            "Epoch 711/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.2303 - val_loss: 1.2267\n",
            "Epoch 712/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.2275 - val_loss: 1.2118\n",
            "Epoch 713/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.2215 - val_loss: 1.2074\n",
            "Epoch 714/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.2084 - val_loss: 1.1968\n",
            "Epoch 715/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1942 - val_loss: 1.1834\n",
            "Epoch 716/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1838 - val_loss: 1.1778\n",
            "Epoch 717/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1660 - val_loss: 1.1563\n",
            "Epoch 718/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1695 - val_loss: 1.1746\n",
            "Epoch 719/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1692 - val_loss: 1.1638\n",
            "Epoch 720/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1588 - val_loss: 1.1538\n",
            "Epoch 721/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1491 - val_loss: 1.1445\n",
            "Epoch 722/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1402 - val_loss: 1.1359\n",
            "Epoch 723/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1319 - val_loss: 1.1279\n",
            "Epoch 724/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1241 - val_loss: 1.1204\n",
            "Epoch 725/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1169 - val_loss: 1.1134\n",
            "Epoch 726/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1101 - val_loss: 1.1068\n",
            "Epoch 727/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1037 - val_loss: 1.1006\n",
            "Epoch 728/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.0977 - val_loss: 1.0949\n",
            "Epoch 729/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.0921 - val_loss: 1.0894\n",
            "Epoch 730/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.0869 - val_loss: 1.0843\n",
            "Epoch 731/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.0819 - val_loss: 1.0795\n",
            "Epoch 732/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.0773 - val_loss: 1.0750\n",
            "Epoch 733/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.0729 - val_loss: 1.0708\n",
            "Epoch 734/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.0688 - val_loss: 1.0668\n",
            "Epoch 735/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.0650 - val_loss: 1.0631\n",
            "Epoch 736/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.0613 - val_loss: 1.0596\n",
            "Epoch 737/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.0579 - val_loss: 1.0562\n",
            "Epoch 738/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.0547 - val_loss: 1.0531\n",
            "Epoch 739/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.0516 - val_loss: 1.0502\n",
            "Epoch 740/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.0480 - val_loss: 1.0688\n",
            "Epoch 741/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1077 - val_loss: 1.1428\n",
            "Epoch 742/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1688 - val_loss: 1.1817\n",
            "Epoch 743/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1886 - val_loss: 1.1900\n",
            "Epoch 744/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1816 - val_loss: 1.1691\n",
            "Epoch 745/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1630 - val_loss: 1.1606\n",
            "Epoch 746/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1511 - val_loss: 1.1479\n",
            "Epoch 747/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1396 - val_loss: 1.1396\n",
            "Epoch 748/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1361 - val_loss: 1.1298\n",
            "Epoch 749/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1250 - val_loss: 1.1212\n",
            "Epoch 750/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1188 - val_loss: 1.1062\n",
            "Epoch 751/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1112 - val_loss: 1.1094\n",
            "Epoch 752/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1072 - val_loss: 1.1087\n",
            "Epoch 753/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1118 - val_loss: 1.1049\n",
            "Epoch 754/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1069 - val_loss: 1.1081\n",
            "Epoch 755/1000\n",
            "82/82 [==============================] - 55s 674ms/step - loss: 1.1038 - val_loss: 1.1009\n",
            "Epoch 756/1000\n",
            "82/82 [==============================] - 55s 675ms/step - loss: 1.1403 - val_loss: 1.1658\n",
            "Epoch 757/1000\n",
            "15/82 [====>.........................] - ETA: 40s - loss: 1.1625Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svI58BCBILqK",
        "outputId": "ed4decdb-104c-4ae8-a31f-c7c34369f95e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "# loss 그래프\n",
        "plt.figure(\"val_graph\")\n",
        "plt.plot(history.history['loss'], 'y', label='loss')\n",
        "plt.plot(history.history['val_loss'],'b', label='val_loss')\n",
        "\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-74d60033f4b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# loss 그래프\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"val_graph\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'y'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBbAU8e7o_Uh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}